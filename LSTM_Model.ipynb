{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8ALdE4JmiJRB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils import resample\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2TSqIBOiRzr",
        "outputId": "9d4a18e1-3cdc-4b82-be86-46d85a1c3312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  label  id\n",
            "0  [16, 231, 543, 5, 15, 43, 8282, 94, 231, 1129,...      1   0\n",
            "1  [16, 4046, 138, 10, 2, 1809, 2007, 3763, 14, 4...      1   1\n",
            "2  [1108, 16550, 3, 6168, 3, 160, 284, 19, 49, 46...      1   2\n",
            "3  [1802, 27, 16, 25, 48, 451, 632, 3, 2, 2164, 2...      1   3\n",
            "4  [16, 19, 302, 93, 97, 43, 952, 118, 1, 16, 528...      1   4\n",
            "                                                text  label    id\n",
            "0  [12, 920, 7, 1266, 28, 9884, 1640, 116, 11, 13...      1  5000\n",
            "1  [783, 397, 253, 5797, 9379, 22, 793, 11838, 10...      1  5001\n",
            "2  [888, 14851, 323, 9, 27, 1377, 584, 195, 3, 13...      1  5002\n",
            "3  [228, 1161, 5815, 379, 9, 941, 10, 2, 316, 4, ...      1  5003\n",
            "4  [736, 19, 37, 813, 45, 6723, 27, 626, 8, 2, 34...      1  5004\n"
          ]
        }
      ],
      "source": [
        "#  Load the data\n",
        "domain1_train_data = pd.read_json(\"domain1_train_data.json\", lines=True)\n",
        "domain2_train_data = pd.read_json(\"domain2_train_data.json\", lines=True)\n",
        "\n",
        "\n",
        "print(domain1_train_data.head())\n",
        "print(domain2_train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pnY4Ur4fjTQA"
      },
      "outputs": [],
      "source": [
        "# get machine and human data\n",
        "machine = domain2_train_data[domain2_train_data['label'] == 0]\n",
        "human = domain2_train_data[domain2_train_data['label'] == 1]\n",
        "\n",
        "# count the number of samples in each class\n",
        "n_machine = len(machine)\n",
        "n_human = len(human)\n",
        "\n",
        "# if the number of samples in 'machine' is greater than the number of samples in 'human'\n",
        "if n_machine > n_human:\n",
        "    machine = machine.sample(n_human)\n",
        "\n",
        "# combine the balanced data\n",
        "domain2_train_data_balanced = pd.concat([machine, human])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5H5g6kDxjaTa"
      },
      "outputs": [],
      "source": [
        "domain2_train_data_balanced = domain2_train_data_balanced.drop(columns='id')\n",
        "domain1_train_data = domain1_train_data.drop(columns='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xm4kixa9jkVM"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3U8vBmwaksvp"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "epochs = 35\n",
        "embedding_dim = 20\n",
        "hidden_dim = 50\n",
        "max_len = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5d6nw8RVkgKN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def seq_collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for  _text, _label in batch:\n",
        "        label_list.append(_label)\n",
        "        text_list.append(sequence_pipeline(_text))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
        "\n",
        "    # Truncate each sequence\n",
        "    padded_sequences = []\n",
        "    for seq in text_list:\n",
        "        # Truncate if longer than max_len\n",
        "        padded_seq = seq[:max_len]\n",
        "         # Pad if shorter\n",
        "        padded_seq += [padding_index] * (max_len - len(padded_seq))\n",
        "        padded_sequences.append(torch.tensor(padded_seq))\n",
        "    text_list = torch.stack(padded_sequences)\n",
        "    # Stack all sequences into a single tensor\n",
        "    return text_list.to(device), label_list.reshape(-1, 1).to(device)\n",
        "\n",
        "def seq_collate_test(batch):\n",
        "    text_list = []\n",
        "    for  _text in batch:\n",
        "        text_list.append(sequence_pipeline(_text))\n",
        "\n",
        "    # Pad or truncate each sequence\n",
        "    padded_sequences = []\n",
        "    for seq in text_list:\n",
        "        # Truncate if longer than max_len\n",
        "        padded_seq = seq[:max_len]\n",
        "         # Pad if shorter\n",
        "        padded_seq += [padding_index] * (max_len - len(padded_seq))\n",
        "        padded_sequences.append(torch.tensor(padded_seq))\n",
        "    text_list = torch.stack(padded_sequences)\n",
        "    # Stack all sequences into a single tensor\n",
        "    return text_list.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UrnjIwdEj4k4"
      },
      "outputs": [],
      "source": [
        "class SimpleLSTMNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, padding_idx):\n",
        "        super().__init__()\n",
        "        self.embedding  = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.forward_layer = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        h0 = torch.zeros((1, batch_size, hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((1, batch_size, hidden_dim)).to(device)\n",
        "        hidden = (h0, c0)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        h0 = self.init_hidden(x.shape[0])\n",
        "        output, (hidden, cell) = self.lstm_layer(embedded, h0)\n",
        "\n",
        "        hidden = hidden[-1, :, :]\n",
        "        logits = torch.sigmoid(self.forward_layer(hidden))\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oOxjIEJTkIq4"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch  == size - 1:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P7lyLPJ5kNFF"
      },
      "outputs": [],
      "source": [
        "def test_without_y(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    arr = []\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X in dataloader:\n",
        "            pred = model(X)\n",
        "            result = (pred>0.5).float()\n",
        "            arr.append(result.data.cpu().numpy())\n",
        "    return arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dvCg1JdpkSdX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    test_preds = []\n",
        "    test_targets = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            preds = torch.argmax(pred , dim=1)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            result = (pred>0.5).float()\n",
        "            test_preds.extend(result.tolist())\n",
        "            test_targets.extend(y.tolist())\n",
        "            correct += (result == y).type(torch.float).sum().item()\n",
        "    class_report = classification_report(test_targets, test_preds)\n",
        "    print(\"Classification Report:\")\n",
        "    print(class_report)\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi7nq1IDnVPA",
        "outputId": "2a4c7e6f-0b94-476e-eb43-57cbd08a0d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domain2_train_data_balanced_upampled = resample(domain2_train_data_balanced,\n",
        "                replace=True,\n",
        "                n_samples=len(domain1_train_data),\n",
        "                random_state=42)\n",
        "\n",
        "combined_data = pd.concat([domain1_train_data, domain2_train_data_balanced_upampled])\n",
        "X = combined_data['text']\n",
        "y = combined_data['label']\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "sequence_pipeline = lambda x: vocab(word_tokenize(x))\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2, shuffle=True)\n",
        "\n",
        "\n",
        "X_train_str = [' '.join(map(str, lst)) for lst in X_train]\n",
        "train_iter = X_train_str\n",
        "def yield_tokens(data_iter):\n",
        "    for line in data_iter:\n",
        "        yield line.strip().split()\n",
        "\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=('<unk>', '<pad>'))\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "padding_index = vocab['0']\n",
        "sequence_pipeline = lambda x: vocab(word_tokenize(x))\n",
        "\n",
        "vectorizer_rnn = CountVectorizer(tokenizer=word_tokenize, vocabulary=vocab.get_stoi(), lowercase=True, ngram_range=(1,2))\n",
        "\n",
        "X_val_str = [' '.join(map(str, lst)) for lst in X_val]\n",
        "X_train_vec = vectorizer_rnn.fit_transform(X_train_str).toarray()\n",
        "X_val_vec = vectorizer_rnn.transform(X_val_str).toarray()\n",
        "test_data = pd.read_json('test_data.json', lines=True)\n",
        "test_texts = [' '.join(map(str, lst)) for lst in test_data['text']]\n",
        "X_test = vectorizer_rnn.transform(test_texts).toarray()\n",
        "vocab_size = X_train_vec.shape[1]\n",
        "\n",
        "\n",
        "train_dl_LSTM = DataLoader(list(zip(X_train_str, y_train)), batch_size= batch_size, collate_fn=seq_collate_batch, shuffle=True)\n",
        "val_dl_LSTM = DataLoader(list(zip(X_val_str, y_val)), batch_size= batch_size, collate_fn=seq_collate_batch)\n",
        "test_dl_LSTM = DataLoader(list(test_texts), batch_size = batch_size, collate_fn=seq_collate_test)\n",
        "\n",
        "LSTM_Model = SimpleLSTMNetwork(vocab_size, embedding_dim, padding_index).to(device)\n",
        "print(LSTM_Model)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(LSTM_Model.parameters(), lr=0.001)\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train(train_dl_LSTM , LSTM_Model, loss_fn, optimizer)\n",
        "    test(val_dl_LSTM, LSTM_Model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "print(\"final test:\")\n",
        "predictions_LSTM_Model = test_without_y(test_dl_LSTM, LSTM_Model, loss_fn)\n",
        "\n",
        "predictions_LSTM_Model_list = []\n",
        "for batch in predictions_LSTM_Model:\n",
        "    for x in batch:\n",
        "        predictions_LSTM_Model_list.append(x[0])\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': range(len(predictions_LSTM_Model_list)),\n",
        "    'class': predictions_LSTM_Model_list\n",
        "    })\n",
        "submission.to_csv('results/LSTMPytorchsModel.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEq-W2DJ1zaA",
        "outputId": "8d533ded-8e74-431f-f456-7e7478bd7059"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleLSTMNetwork(\n",
            "  (embedding): Embedding(38431, 20, padding_idx=0)\n",
            "  (lstm_layer): LSTM(20, 50, batch_first=True)\n",
            "  (forward_layer): Linear(in_features=50, out_features=1, bias=True)\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.55      0.72      0.63       983\n",
            "         1.0       0.62      0.44      0.52      1017\n",
            "\n",
            "    accuracy                           0.58      2000\n",
            "   macro avg       0.59      0.58      0.57      2000\n",
            "weighted avg       0.59      0.58      0.57      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 0.678957 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.63      0.67       983\n",
            "         1.0       0.68      0.74      0.71      1017\n",
            "\n",
            "    accuracy                           0.69      2000\n",
            "   macro avg       0.69      0.69      0.69      2000\n",
            "weighted avg       0.69      0.69      0.69      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.595560 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.78      0.76       983\n",
            "         1.0       0.78      0.75      0.76      1017\n",
            "\n",
            "    accuracy                           0.76      2000\n",
            "   macro avg       0.76      0.76      0.76      2000\n",
            "weighted avg       0.76      0.76      0.76      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.524967 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.82      0.80       983\n",
            "         1.0       0.82      0.79      0.80      1017\n",
            "\n",
            "    accuracy                           0.80      2000\n",
            "   macro avg       0.80      0.80      0.80      2000\n",
            "weighted avg       0.80      0.80      0.80      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 80.3%, Avg loss: 0.501510 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.84      0.82       983\n",
            "         1.0       0.84      0.79      0.81      1017\n",
            "\n",
            "    accuracy                           0.81      2000\n",
            "   macro avg       0.82      0.81      0.81      2000\n",
            "weighted avg       0.82      0.81      0.81      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.513187 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.86      0.84       983\n",
            "         1.0       0.86      0.81      0.83      1017\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.84      0.84      0.83      2000\n",
            "weighted avg       0.84      0.83      0.83      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.495254 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.86      0.84       983\n",
            "         1.0       0.86      0.82      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.84      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.515634 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.89      0.84       983\n",
            "         1.0       0.88      0.77      0.82      1017\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.83      0.83      0.83      2000\n",
            "weighted avg       0.84      0.83      0.83      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 82.9%, Avg loss: 0.595530 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.89      0.84       983\n",
            "         1.0       0.87      0.77      0.82      1017\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.83      0.83      0.83      2000\n",
            "weighted avg       0.83      0.83      0.83      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.600773 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.87      0.86       983\n",
            "         1.0       0.87      0.86      0.86      1017\n",
            "\n",
            "    accuracy                           0.86      2000\n",
            "   macro avg       0.86      0.86      0.86      2000\n",
            "weighted avg       0.86      0.86      0.86      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 86.2%, Avg loss: 0.544358 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.88      0.85       983\n",
            "         1.0       0.88      0.82      0.85      1017\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.0%, Avg loss: 0.537147 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.89      0.85       983\n",
            "         1.0       0.88      0.80      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.85      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.615949 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.89      0.85       983\n",
            "         1.0       0.88      0.80      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.85      0.85      0.84      2000\n",
            "weighted avg       0.85      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.591568 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.87      0.85       983\n",
            "         1.0       0.87      0.83      0.85      1017\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.0%, Avg loss: 0.583586 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.90      0.84       983\n",
            "         1.0       0.89      0.76      0.82      1017\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.84      0.83      0.83      2000\n",
            "weighted avg       0.84      0.83      0.83      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.650600 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.88      0.84       983\n",
            "         1.0       0.87      0.78      0.83      1017\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.84      0.83      0.83      2000\n",
            "weighted avg       0.84      0.83      0.83      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.589415 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.89      0.85       983\n",
            "         1.0       0.88      0.80      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.84      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.619043 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.89      0.85       983\n",
            "         1.0       0.88      0.80      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.85      0.84      0.84      2000\n",
            "weighted avg       0.85      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.602050 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.89      0.84       983\n",
            "         1.0       0.88      0.77      0.82      1017\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.84      0.83      0.83      2000\n",
            "weighted avg       0.84      0.83      0.83      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.1%, Avg loss: 0.630109 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.87      0.85       983\n",
            "         1.0       0.87      0.81      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.84      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.609244 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.89      0.85       983\n",
            "         1.0       0.88      0.81      0.84      1017\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.585433 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.88      0.85       983\n",
            "         1.0       0.88      0.81      0.84      1017\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 0.611551 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.89      0.84       983\n",
            "         1.0       0.88      0.79      0.83      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.84      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.649403 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.88      0.85       983\n",
            "         1.0       0.88      0.80      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.84      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.635381 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.88      0.85       983\n",
            "         1.0       0.88      0.81      0.84      1017\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 0.595039 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.89      0.84       983\n",
            "         1.0       0.88      0.77      0.82      1017\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.83      0.83      0.83      2000\n",
            "weighted avg       0.83      0.83      0.83      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.648455 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.88      0.85       983\n",
            "         1.0       0.88      0.82      0.85      1017\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 85.2%, Avg loss: 0.617169 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.88      0.85       983\n",
            "         1.0       0.87      0.81      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.85      0.85      0.84      2000\n",
            "weighted avg       0.85      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.644340 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.87      0.85       983\n",
            "         1.0       0.87      0.82      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.85      0.85      0.84      2000\n",
            "weighted avg       0.85      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 0.635965 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.87      0.84       983\n",
            "         1.0       0.87      0.81      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.84      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.629699 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.87      0.85       983\n",
            "         1.0       0.87      0.82      0.84      1017\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.6%, Avg loss: 0.660139 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.87      0.84       983\n",
            "         1.0       0.87      0.80      0.83      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.84      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.654892 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.77      0.89      0.83       983\n",
            "         1.0       0.87      0.75      0.81      1017\n",
            "\n",
            "    accuracy                           0.82      2000\n",
            "   macro avg       0.82      0.82      0.82      2000\n",
            "weighted avg       0.82      0.82      0.82      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.613421 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.87      0.84       983\n",
            "         1.0       0.87      0.81      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.84      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.627230 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.87      0.84       983\n",
            "         1.0       0.87      0.81      0.84      1017\n",
            "\n",
            "    accuracy                           0.84      2000\n",
            "   macro avg       0.84      0.84      0.84      2000\n",
            "weighted avg       0.84      0.84      0.84      2000\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.646521 \n",
            "\n",
            "Done!\n",
            "final test:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QhFKKq1C1ha-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0thfZ874iDBy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}