{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ALdE4JmiJRB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils import resample\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K2TSqIBOiRzr"
      },
      "outputs": [],
      "source": [
        "#  Load the data\n",
        "domain1_train_data = pd.read_json(\"domain1_train_data.json\", lines=True)\n",
        "domain2_train_data = pd.read_json(\"domain2_train_data.json\", lines=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pnY4Ur4fjTQA"
      },
      "outputs": [],
      "source": [
        "# get machine and human data\n",
        "machine = domain2_train_data[domain2_train_data['label'] == 0]\n",
        "human = domain2_train_data[domain2_train_data['label'] == 1]\n",
        "\n",
        "# count the number of samples in each class\n",
        "n_machine = len(machine)\n",
        "n_human = len(human)\n",
        "\n",
        "# if the number of samples in 'machine' is greater than the number of samples in 'human'\n",
        "if n_machine > n_human:\n",
        "    machine = machine.sample(n_human)\n",
        "\n",
        "# combine the balanced data\n",
        "domain2_train_data_balanced = pd.concat([machine, human])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5H5g6kDxjaTa"
      },
      "outputs": [],
      "source": [
        "#drop id columns\n",
        "domain2_train_data_balanced = domain2_train_data_balanced.drop(columns='id')\n",
        "domain1_train_data = domain1_train_data.drop(columns='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xm4kixa9jkVM"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3U8vBmwaksvp"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "epochs = 10\n",
        "embedding_dim = 15\n",
        "hidden_dim = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5d6nw8RVkgKN"
      },
      "outputs": [],
      "source": [
        "def bow_collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for  _text, _label in batch:\n",
        "        label_list.append(_label)\n",
        "        text_list.append(_text)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
        "    text_list = torch.tensor(text_list, dtype=torch.float32)\n",
        "\n",
        "    return text_list.to(device), label_list.reshape(-1, 1).to(device)\n",
        "\n",
        "def test_batch(batch):\n",
        "    text_list = []\n",
        "    for  _text in batch:\n",
        "        text_list.append(_text)\n",
        "    text_list = torch.tensor(text_list, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    return text_list.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sX9oOnFekC_h"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.hidden_layer(x))\n",
        "        out = torch.sigmoid(self.output_layer(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oOxjIEJTkIq4"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch  == size - 1:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P7lyLPJ5kNFF"
      },
      "outputs": [],
      "source": [
        "def test_without_y(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    arr = []\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X in dataloader:\n",
        "            pred = model(X)\n",
        "            result = (pred>0.5).float()\n",
        "            arr.append(result.data.cpu().numpy())\n",
        "    return arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dvCg1JdpkSdX"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    test_preds = []\n",
        "    test_targets = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            preds = torch.argmax(pred , dim=1)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            result = (pred>0.5).float()\n",
        "            test_preds.extend(result.tolist())\n",
        "            test_targets.extend(y.tolist())\n",
        "            correct += (result == y).type(torch.float).sum().item()\n",
        "    class_report = classification_report(test_targets, test_preds)\n",
        "    print(\"Classification Report:\")\n",
        "    print(class_report)\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Oi7nq1IDnVPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310f0165-b5ca-479b-849f-54f750c1d99c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domain2_train_data_balanced_upampled = resample(domain2_train_data_balanced,\n",
        "                replace=True,\n",
        "                n_samples=len(domain1_train_data),\n",
        "                random_state=42)\n",
        "\n",
        "combined_data = pd.concat([domain1_train_data, domain2_train_data_balanced_upampled])\n",
        "X = combined_data['text']\n",
        "y = combined_data['label']\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "sequence_pipeline = lambda x: vocab(word_tokenize(x))\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2, shuffle=True)\n",
        "X_train_str_MLP = [' '.join(map(str, lst)) for lst in X_train]\n",
        "X_val_str_MLP = [' '.join(map(str, lst)) for lst in X_val]\n",
        "train_iter = X_train_str_MLP\n",
        "def yield_tokens(data_iter):\n",
        "    for line in data_iter:\n",
        "        yield line.strip().split()\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=('<unk>', '<pad>'))\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "padding_index = vocab['0']\n",
        "vectorizer_mlp = TfidfVectorizer(tokenizer=word_tokenize, lowercase=True, ngram_range=(1,3))\n",
        "X_train_vec_MLP = vectorizer_mlp.fit_transform(X_train_str_MLP).toarray()\n",
        "X_val_vec_MLP = vectorizer_mlp.transform(X_val_str_MLP).toarray()\n",
        "test_data = pd.read_json('test_data.json', lines=True)\n",
        "test_texts = [' '.join(map(str, lst)) for lst in test_data['text']]\n",
        "X_test = vectorizer_mlp.transform(test_texts).toarray()\n",
        "\n",
        "\n",
        "train_dl_MLP = DataLoader(list(zip(X_train_vec_MLP, y_train)), batch_size=batch_size, collate_fn=bow_collate_batch, shuffle=True)\n",
        "val_dl_MLP = DataLoader(list(zip(X_val_vec_MLP, y_val)), batch_size=batch_size, collate_fn=bow_collate_batch)\n",
        "test_dl_MLP = DataLoader(list(X_test), batch_size= batch_size, collate_fn=test_batch)\n",
        "vocab_size = X_train_vec_MLP.shape[1]\n",
        "MLP_Model = MLP(vocab_size, 30, 1).to(device)\n",
        "print(MLP_Model)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(MLP_Model.parameters(), lr=0.001)\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train(train_dl_MLP , MLP_Model, loss_fn, optimizer)\n",
        "    test(val_dl_MLP, MLP_Model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "print(\"final test:\")\n",
        "predictions_MLP = test_without_y(test_dl_MLP, MLP_Model, loss_fn)\n",
        "\n",
        "predictions_MLP_Model = []\n",
        "for batch in predictions_MLP:\n",
        "    for x in batch:\n",
        "        predictions_MLP_Model.append(x[0])\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': range(len(predictions_MLP_Model)),\n",
        "    'class': predictions_MLP_Model\n",
        "    })\n",
        "submission.to_csv('results/MLP_Model.csv', index=False)"
      ],
      "metadata": {
        "id": "DXaRFg-xagCO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}